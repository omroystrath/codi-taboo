model_args:
  model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  lora_r: 128
  lora_alpha: 32
  lora_init: true
  token: null
  full_precision: true

data_args:
  data_names:
    - "taboo:data/combined-train.jsonl"
  max_samples: null

wandb:
  project: "codi-taboo"
  run_name: "codi_taboo_gold_resume"

hub:
  push_to_hub: false
  account: ""
  private_repo: false

training_args:
  output_dir: "./models/codi_taboo_gold_resumed"
  expt_name: "codi_taboo_gold_resumed"
  logging_dir: "./models/codi_taboo_gold_resumed/logs"
  logging_steps: 10
  seed: 42
  model_max_length: 512
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  bf16: true
  max_steps: -1
  num_train_epochs: 6
  learning_rate: 4e-4
  max_grad_norm: 2.0
  save_strategy: "epoch"
  save_total_limit: 5
  save_safetensors: false
  weight_decay: 0.1
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  do_train: true
  report_to: "wandb"
  num_latent: 6
  logging_strategy: "steps"
  use_prj: true
  prj_dim: 2048
  prj_dropout: 0.0
  distill_loss_div_std: true
  exp_mode: false
  exp_data_num: 200
  remove_eos: false
  distill_loss_factor: 20
  ref_loss_factor: 1.0
  print_ref_model_stats: false
  print_loss: false
  max_token_num: 512
  dataloader_num_workers: 4
  ddp_find_unused_parameters: false
  sft_loss_factor: 1.0
  include_last_cot: false
  ce_loss_factor: 1.0
  answer_only: true
  restore_from: "./models/codi_taboo_gold/codi_taboo_gold/Llama-3.2-1B-Instruct/ep_10/lr_0.0008/seed_42/checkpoint-308/pytorch_model.bin"
