# CODI Taboo Training Config â€” v4 (fresh start from restore_from)

model_args:
  model_name_or_path: "meta-llama/Llama-3.2-1B-Instruct"
  lora_r: 128
  lora_alpha: 32
  lora_init: true
  token: null
  full_precision: true

data_args:
  data_names:
    - "taboo:data/combined-train-v4.jsonl"
  max_samples: null

wandb:
  project: "codi-taboo"
  run_name: "codi_taboo_gold_v4"

hub:
  push_to_hub: false
  account: ""
  private_repo: false

training_args:
  output_dir: "./models/codi_taboo_gold_v4"
  expt_name: "codi_taboo_gold_v4"
  logging_dir: "./models/codi_taboo_gold_v4/logs"
  logging_steps: 10
  seed: 42

  model_max_length: 512
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  bf16: true

  max_steps: -1
  num_train_epochs: 10
  learning_rate: 8e-4
  max_grad_norm: 2.0

  save_strategy: "epoch"
  save_total_limit: 5
  save_safetensors: false

  weight_decay: 0.1
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"

  do_train: true
  report_to: "wandb"
  logging_strategy: "steps"

  num_latent: 6
  use_prj: true
  prj_dim: 2048
  prj_dropout: 0.0

  distill_loss_div_std: true
  distill_loss_factor: 20
  ref_loss_factor: 1.0

  sft_loss_factor: 1.0
  ce_loss_factor: 1.0

  exp_mode: false
  exp_data_num: 200

  remove_eos: false
  include_last_cot: false
  answer_only: true

  print_ref_model_stats: false
  print_loss: false
  max_token_num: 512
  dataloader_num_workers: 4
  ddp_find_unused_parameters: false

  # Start from pretrained CODI checkpoint (fresh run; no resume)
  restore_from: "./checkpoints/codi_llama1b-answer_only/pytorch_model.bin"
